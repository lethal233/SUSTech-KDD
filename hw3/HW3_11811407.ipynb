{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn import metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "df1 = pd.read_csv(\"./HW3_1_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# show the head rows of the data\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# drop the rows that have null values\n",
    "df1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# to fetch the x and y data\n",
    "X1 = df1.iloc[:, 0].values.reshape(-1, 1)\n",
    "y1 = df1.iloc[:, -1].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot the scatter picture, and to see how many clusters can be\n",
    "plt.scatter(X1, y1)\n",
    "# 7 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the eps and min_samples in the DBSCAN parameters list\n",
    "\n",
    "# rs = []\n",
    "# eps = np.arange(0.1, 4, 0.1)\n",
    "# min_samples = np.arange(2, 20, 1)\n",
    "#\n",
    "# best_score = 0\n",
    "# best_score_eps = 0\n",
    "# best_score_min_samples = 0\n",
    "#\n",
    "# for i in eps:\n",
    "#     for j in min_samples:\n",
    "#         try:\n",
    "#             db = DBSCAN(eps=i, min_samples=j, n_jobs=-1).fit(df1)\n",
    "#             labels = db.labels_\n",
    "#             k = metrics.silhouette_score(df1, labels) # to get the current silhouette_score\n",
    "#             ratio = len(labels[labels[:] == -1]) / len(labels)  # to calculate the ratio of noise and the total point\n",
    "#             n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)  # obtain the number of clusters\n",
    "#             rs.append([i, j, k, ratio, n_clusters_])\n",
    "#\n",
    "#             if k > best_score:\n",
    "#                 best_score = k\n",
    "#                 best_score_eps = i\n",
    "#                 best_score_min_samples = j\n",
    "#             else:\n",
    "#                 db = ''\n",
    "#         except Exception:\n",
    "#             db = ''\n",
    "# rs = pd.DataFrame(rs)\n",
    "#rs.columns = ['eps', 'min_samples', 'score', 'ratio', 'n_clusters']\n",
    "# sns.relplot(x=\"eps\", y=\"min_samples\", size='score', data=rs)\n",
    "# sns.relplot(x=\"eps\", y=\"min_samples\", size='ratio', data=rs)\n",
    "# print(best_score_eps,best_score_min_samples)\n",
    "# to get the best parameters of the DBSCAN is 1.3 and 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set the DBSCAN parameters according to the above\n",
    "dbscan1 = DBSCAN(eps=1.3, min_samples=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# to plot the results of the classification\n",
    "y_pred1 = dbscan1.fit_predict(df1)\n",
    "plt.scatter(X1, y1, c=y_pred1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use SpectralClustering to predict and classification\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# n_cluster is 7 as we can see\n",
    "y_p = SpectralClustering(n_clusters=7).fit_predict(df1)\n",
    "plt.scatter(X1, y1, c=y_p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "df2 = pd.read_csv(\"./HW3_2_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# show the head rows of the data\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# describe the data\n",
    "df2.describe()\n",
    "# we can see that the data is biased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CUST_ID is not useful, drop it\n",
    "df2.drop(['CUST_ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Because the credit limit has just 1 missing value, so dropping it makes no difference\n",
    "df2.dropna(subset=['CREDIT_LIMIT'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# It seems that no columns have relationships with this column so use median value to replace it\n",
    "df2['MINIMUM_PAYMENTS'].fillna(df2['MINIMUM_PAYMENTS'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot the distribution of each column\n",
    "plt.figure(figsize=(20, 30))\n",
    "for i, col in enumerate(df2.columns):\n",
    "    ax = plt.subplot(9, 2, i + 1)\n",
    "    sns.kdeplot(df2[col], ax=ax)\n",
    "    plt.xlabel(col)\n",
    "plt.show()\n",
    "# I have found that many of the columns are skewed, so we must handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# to take the log value of these columns to evaluate it normally \n",
    "cols = ['BALANCE', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'ONEOFF_PURCHASES_FREQUENCY',\n",
    "        'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n",
    "        'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT']\n",
    "# here do not need to add `PURCHASES` since the result is not good\n",
    "for col in cols:\n",
    "    df2[col] = np.log(1 + df2[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 25))\n",
    "for i, col in enumerate(cols):\n",
    "    ax = plt.subplot(6, 2, i + 1)\n",
    "    sns.kdeplot(df2[col], ax=ax)\n",
    "    plt.xlabel(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# since the dimension of the data is so large, we need to take the important information of the data\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA helps to decrease the dimensions\n",
    "# using `mle` to fit automatically\n",
    "pca = PCA(n_components='mle')\n",
    "X_reduce = pca.fit_transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the best k for clustering\n",
    "km = []\n",
    "innertia_m = []\n",
    "for i in range(1, 10):\n",
    "    km.append(KMeans(n_clusters=i, random_state=0).fit(X_reduce))\n",
    "    innertia_m.append(km[i - 1].inertia_)\n",
    "plt.scatter(range(1, 10), innertia_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# in the picture we can see that the best possible k is 3\n",
    "# and we use 3 for clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans.fit(X_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df2['LABEL'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# to recover the data\n",
    "for col in cols:\n",
    "    df2[col] = np.exp(df2[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# draw the picture between the `oneoff_puchases` and the `purchases`\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "sns.scatterplot(palette='Accent', data=df2, x='ONEOFF_PURCHASES', y='PURCHASES', hue='LABEL')\n",
    "plt.title('Clustered by one-off-purchases and total-purchases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# draw the picture between the `credit_limit` and the `purchases`\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "sns.scatterplot(palette='Accent', data=df2, x='CREDIT_LIMIT', y='PURCHASES', hue='LABEL')\n",
    "plt.title('Clustered by credit-limit and total-purchases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# draw the picture between the `oneoff_purchases_frequency` and the `purchases`\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "sns.scatterplot(palette='Accent', data=df2, x='ONEOFF_PURCHASES_FREQUENCY', y='PURCHASES', hue='LABEL')\n",
    "plt.title('Clustered by Oneoff-purchases-frequency and total-purchases')\n",
    "plt.show()\n",
    "\n",
    "# we can see that the clustering makes sense"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}